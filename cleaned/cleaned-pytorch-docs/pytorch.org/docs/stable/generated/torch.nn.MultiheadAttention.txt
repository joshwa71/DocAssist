

  * Get Started
  * Ecosystem
  * Mobile
  * Blog
  * Tutorials
  * Docs 

PyTorch

torchaudio

torchtext

torchvision

torcharrow

TorchData

TorchRec

TorchServe

TorchX

PyTorch on XLA Devices

  * Resources 

About

Learn about PyTorch’s features and capabilities

PyTorch Foundation

Learn about the PyTorch foundation

Community

Join the PyTorch developer community to contribute, learn, and get your
questions answered.

Community Stories

Learn how our community solves real, everyday machine learning problems with
PyTorch.

Developer Resources

Find resources and get questions answered

Events

Find events, webinars, and podcasts

Forums

A place to discuss PyTorch code, issues, install, research

Models (Beta)

Discover, publish, and reuse pre-trained models

  * GitHub

Table of Contents

2.1 ▼

Community

  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers

Developer Notes

  * CUDA Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ

Language Bindings

  * C++
  * Javadoc
  * torch::deploy

Python API

  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.optim
  * torch.distributed.tensor.parallel
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Pipeline Parallelism
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch._logging

Libraries

  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices

  * Docs  >
  * torch.nn >
  * MultiheadAttention
  * ![](../_static/images/view-page-source-icon.svg)

Shortcuts

# MultiheadAttention¶

_class _torch.nn.MultiheadAttention( _embed_dim_ , _num_heads_ , _dropout
=0.0_, _bias =True_, _add_bias_kv =False_, _add_zero_attn =False_, _kdim
=None_, _vdim =None_, _batch_first =False_, _device =None_, _dtype
=None_)[source]¶

    

Allows the model to jointly attend to information from different
representation subspaces as described in the paper: Attention Is All You Need.

Multi-Head Attention is defined as:

MultiHead(Q,K,V)=Concat(head1,…,headh)WO\text{MultiHead}(Q, K, V) =
\text{Concat}(head_1,\dots,head_h)W^O
MultiHead(Q,K,V)=Concat(head1​,…,headh​)WO

where headi=Attention(QWiQ,KWiK,VWiV)head_i = \text{Attention}(QW_i^Q, KW_i^K,
VW_i^V)headi​=Attention(QWiQ​,KWiK​,VWiV​).

`nn.MultiHeadAttention` will use the optimized implementations of
`scaled_dot_product_attention()` when possible.

In addition to support for the new `scaled_dot_product_attention()` function,
for speeding up Inference, MHA will use fastpath inference with support for
Nested Tensors, iff:

  * self attention is being computed (i.e., `query`, `key`, and `value` are the same tensor).

  * inputs are batched (3D) with `batch_first==True`

  * Either autograd is disabled (using `torch.inference_mode` or `torch.no_grad`) or no tensor argument `requires_grad`

  * training is disabled (using `.eval()`)

  * `add_bias_kv` is `False`

  * `add_zero_attn` is `False`

  * `batch_first` is `True` and the input is batched

  * `kdim` and `vdim` are equal to `embed_dim`

  * if a NestedTensor is passed, neither `key_padding_mask` nor `attn_mask` is passed

  * autocast is disabled

If the optimized inference fastpath implementation is in use, a NestedTensor
can be passed for `query`/`key`/`value` to represent padding more efficiently
than using a padding mask. In this case, a NestedTensor will be returned, and
an additional speedup proportional to the fraction of the input that is
padding can be expected.

Parameters

    

  * **embed_dim** – Total dimension of the model.

  * **num_heads** – Number of parallel attention heads. Note that `embed_dim` will be split across `num_heads` (i.e. each head will have dimension `embed_dim // num_heads`).

  * **dropout** – Dropout probability on `attn_output_weights`. Default: `0.0` (no dropout).

  * **bias** – If specified, adds bias to input / output projection layers. Default: `True`.

  * **add_bias_kv** – If specified, adds bias to the key and value sequences at dim=0. Default: `False`.

  * **add_zero_attn** – If specified, adds a new batch of zeros to the key and value sequences at dim=1. Default: `False`.

  * **kdim** – Total number of features for keys. Default: `None` (uses `kdim=embed_dim`).

  * **vdim** – Total number of features for values. Default: `None` (uses `vdim=embed_dim`).

  * **batch_first** – If `True`, then the input and output tensors are provided as (batch, seq, feature). Default: `False` (seq, batch, feature).

Examples:

    
    
    >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)
    >>> attn_output, attn_output_weights = multihead_attn(query, key, value)
    

forward( _query_ , _key_ , _value_ , _key_padding_mask =None_, _need_weights
=True_, _attn_mask =None_, _average_attn_weights =True_, _is_causal
=False_)[source]¶

    

Parameters

    

  * **query** ( _Tensor_) – Query embeddings of shape (L,Eq)(L, E_q)(L,Eq​) for unbatched input, (L,N,Eq)(L, N, E_q)(L,N,Eq​) when `batch_first=False` or (N,L,Eq)(N, L, E_q)(N,L,Eq​) when `batch_first=True`, where LLL is the target sequence length, NNN is the batch size, and EqE_qEq​ is the query embedding dimension `embed_dim`. Queries are compared against key-value pairs to produce the output. See “Attention Is All You Need” for more details.

  * **key** ( _Tensor_) – Key embeddings of shape (S,Ek)(S, E_k)(S,Ek​) for unbatched input, (S,N,Ek)(S, N, E_k)(S,N,Ek​) when `batch_first=False` or (N,S,Ek)(N, S, E_k)(N,S,Ek​) when `batch_first=True`, where SSS is the source sequence length, NNN is the batch size, and EkE_kEk​ is the key embedding dimension `kdim`. See “Attention Is All You Need” for more details.

  * **value** ( _Tensor_) – Value embeddings of shape (S,Ev)(S, E_v)(S,Ev​) for unbatched input, (S,N,Ev)(S, N, E_v)(S,N,Ev​) when `batch_first=False` or (N,S,Ev)(N, S, E_v)(N,S,Ev​) when `batch_first=True`, where SSS is the source sequence length, NNN is the batch size, and EvE_vEv​ is the value embedding dimension `vdim`. See “Attention Is All You Need” for more details.

  * **key_padding_mask** ( _Optional_ _[_ _Tensor_ _]_ ) – If specified, a mask of shape (N,S)(N, S)(N,S) indicating which elements within `key` to ignore for the purpose of attention (i.e. treat as “padding”). For unbatched query, shape should be (S)(S)(S). Binary and float masks are supported. For a binary mask, a `True` value indicates that the corresponding `key` value will be ignored for the purpose of attention. For a float mask, it will be directly added to the corresponding `key` value.

  * **need_weights** ( _bool_) – If specified, returns `attn_output_weights` in addition to `attn_outputs`. Set `need_weights=False` to use the optimized `scaled_dot_product_attention` and achieve the best performance for MHA. Default: `True`.

  * **attn_mask** ( _Optional_ _[_ _Tensor_ _]_ ) – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape (L,S)(L, S)(L,S) or (N⋅num_heads,L,S)(N\cdot\text{num\\_heads}, L, S)(N⋅num_heads,L,S), where NNN is the batch size, LLL is the target sequence length, and SSS is the source sequence length. A 2D mask will be broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch. Binary and float masks are supported. For a binary mask, a `True` value indicates that the corresponding position is not allowed to attend. For a float mask, the mask values will be added to the attention weight. If both attn_mask and key_padding_mask are supplied, their types should match.

  * **average_attn_weights** ( _bool_) – If true, indicates that the returned `attn_weights` should be averaged across heads. Otherwise, `attn_weights` are provided separately per head. Note that this flag only has an effect when `need_weights=True`. Default: `True` (i.e. average weights across heads)

  * **is_causal** ( _bool_) – If specified, applies a causal mask as attention mask. Default: `False`. Warning: `is_causal` provides a hint that `attn_mask` is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility.

Return type

    

_Tuple_[ _Tensor_, _Optional_[ _Tensor_]]

Outputs:

    

  * **attn_output** \- Attention outputs of shape (L,E)(L, E)(L,E) when input is unbatched, (L,N,E)(L, N, E)(L,N,E) when `batch_first=False` or (N,L,E)(N, L, E)(N,L,E) when `batch_first=True`, where LLL is the target sequence length, NNN is the batch size, and EEE is the embedding dimension `embed_dim`.

  * **attn_output_weights** \- Only returned when `need_weights=True`. If `average_attn_weights=True`, returns attention weights averaged across heads of shape (L,S)(L, S)(L,S) when input is unbatched or (N,L,S)(N, L, S)(N,L,S), where NNN is the batch size, LLL is the target sequence length, and SSS is the source sequence length. If `average_attn_weights=False`, returns attention weights per head of shape (num_heads,L,S)(\text{num\\_heads}, L, S)(num_heads,L,S) when input is unbatched or (N,num_heads,L,S)(N, \text{num\\_heads}, L, S)(N,num_heads,L,S).

Note

batch_first argument is ignored for unbatched inputs.

merge_masks( _attn_mask_ , _key_padding_mask_ , _query_ )[source]¶

    

Determine mask type and combine masks if necessary. If only one mask is
provided, that mask and the corresponding mask type will be returned. If both
masks are provided, they will be both expanded to shape `(batch_size,
num_heads, seq_len, seq_len)`, combined with logical `or` and mask type 2 will
be returned :param attn_mask: attention mask of shape `(seq_len, seq_len)`,
mask type 0 :param key_padding_mask: padding mask of shape `(batch_size,
seq_len)`, mask type 1 :param query: query embeddings of shape `(batch_size,
seq_len, embed_dim)`

Returns

    

merged mask mask_type: merged mask type (0, 1, or 2)

Return type

    

merged_mask

Next ![](../_static/images/chevron-right-orange.svg)
![](../_static/images/chevron-right-orange.svg) Previous

* * *

(C) Copyright 2023, PyTorch Contributors.

Built with Sphinx using a theme provided by Read the Docs.

  * MultiheadAttention
    * `MultiheadAttention`
      * `MultiheadAttention.forward()`
      * `MultiheadAttention.merge_masks()`

![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)

## Docs

Access comprehensive developer documentation for PyTorch

View Docs

## Tutorials

Get in-depth tutorials for beginners and advanced developers

View Tutorials

## Resources

Find development resources and get your questions answered

View Resources

  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing

  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines

  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn

  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon

  * Terms
  * |
  * Privacy

© Copyright The Linux Foundation. The PyTorch Foundation is a project of The
Linux Foundation. For web site terms of use, trademark policy and other
policies applicable to The PyTorch Foundation please see
www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch
open source project, which has been established as PyTorch Project a Series of
LF Projects, LLC. For policies applicable to the PyTorch Project a Series of
LF Projects, LLC, please see www.lfprojects.org/policies/.

To analyze traffic and optimize your experience, we serve cookies on this
site. By clicking or navigating, you agree to allow our usage of cookies. As
the current maintainers of this site, Facebook’s Cookies Policy applies. Learn
more, including about available controls: Cookies Policy.

![](../_static/images/pytorch-x.svg)

  * Get Started
  * Ecosystem
  * Mobile
  * Blog
  * Tutorials
  * Docs 
    * PyTorch
    * torchaudio
    * torchtext
    * torchvision
    * torcharrow
    * TorchData
    * TorchRec
    * TorchServe
    * TorchX
    * PyTorch on XLA Devices
  * Resources 
    * About
    * PyTorch Foundation
    * Community
    * Community Stories
    * Developer Resources
    * Events
    * Forums
    * Models (Beta)
  * Github

