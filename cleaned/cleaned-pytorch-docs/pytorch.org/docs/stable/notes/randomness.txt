

  * Get Started
  * Ecosystem
  * Mobile
  * Blog
  * Tutorials
  * Docs 

PyTorch

torchaudio

torchtext

torchvision

torcharrow

TorchData

TorchRec

TorchServe

TorchX

PyTorch on XLA Devices

  * Resources 

About

Learn about PyTorch’s features and capabilities

PyTorch Foundation

Learn about the PyTorch foundation

Community

Join the PyTorch developer community to contribute, learn, and get your
questions answered.

Community Stories

Learn how our community solves real, everyday machine learning problems with
PyTorch.

Developer Resources

Find resources and get questions answered

Events

Find events, webinars, and podcasts

Forums

A place to discuss PyTorch code, issues, install, research

Models (Beta)

Discover, publish, and reuse pre-trained models

  * GitHub

Table of Contents

2.1 ▼

Community

  * PyTorch Governance | Build + CI
  * PyTorch Contribution Guide
  * PyTorch Design Philosophy
  * PyTorch Governance | Mechanics
  * PyTorch Governance | Maintainers

Developer Notes

  * CUDA Automatic Mixed Precision examples
  * Autograd mechanics
  * Broadcasting semantics
  * CPU threading and TorchScript inference
  * CUDA semantics
  * Distributed Data Parallel
  * Extending PyTorch
  * Extending torch.func with autograd.Function
  * Frequently Asked Questions
  * Gradcheck mechanics
  * HIP (ROCm) semantics
  * Features for large-scale deployments
  * Modules
  * MPS backend
  * Multiprocessing best practices
  * Numerical accuracy
  * Reproducibility
  * Serialization semantics
  * Windows FAQ

Language Bindings

  * C++
  * Javadoc
  * torch::deploy

Python API

  * torch
  * torch.nn
  * torch.nn.functional
  * torch.Tensor
  * Tensor Attributes
  * Tensor Views
  * torch.amp
  * torch.autograd
  * torch.library
  * torch.cpu
  * torch.cuda
  * Understanding CUDA Memory Usage
  * Generating a Snapshot
  * Using the visualizer
  * Snapshot API Reference
  * torch.mps
  * torch.backends
  * torch.export
  * torch.distributed
  * torch.distributed.algorithms.join
  * torch.distributed.elastic
  * torch.distributed.fsdp
  * torch.distributed.optim
  * torch.distributed.tensor.parallel
  * torch.distributed.checkpoint
  * torch.distributions
  * torch.compiler
  * torch.fft
  * torch.func
  * torch.futures
  * torch.fx
  * torch.hub
  * torch.jit
  * torch.linalg
  * torch.monitor
  * torch.signal
  * torch.special
  * torch.overrides
  * torch.package
  * torch.profiler
  * torch.nn.init
  * torch.onnx
  * torch.optim
  * Complex Numbers
  * DDP Communication Hooks
  * Pipeline Parallelism
  * Quantization
  * Distributed RPC Framework
  * torch.random
  * torch.masked
  * torch.nested
  * torch.sparse
  * torch.Storage
  * torch.testing
  * torch.utils
  * torch.utils.benchmark
  * torch.utils.bottleneck
  * torch.utils.checkpoint
  * torch.utils.cpp_extension
  * torch.utils.data
  * torch.utils.jit
  * torch.utils.dlpack
  * torch.utils.mobile_optimizer
  * torch.utils.model_zoo
  * torch.utils.tensorboard
  * Type Info
  * Named Tensors
  * Named Tensors operator coverage
  * torch.__config__
  * torch._logging

Libraries

  * torchaudio
  * TorchData
  * TorchRec
  * TorchServe
  * torchtext
  * torchvision
  * PyTorch on XLA Devices

  * Docs  >
  * Reproducibility
  * ![](../_static/images/view-page-source-icon.svg)

Shortcuts

# Reproducibility¶

Completely reproducible results are not guaranteed across PyTorch releases,
individual commits, or different platforms. Furthermore, results may not be
reproducible between CPU and GPU executions, even when using identical seeds.

However, there are some steps you can take to limit the number of sources of
nondeterministic behavior for a specific platform, device, and PyTorch
release. First, you can control sources of randomness that can cause multiple
executions of your application to behave differently. Second, you can
configure PyTorch to avoid using nondeterministic algorithms for some
operations, so that multiple calls to those operations, given the same inputs,
will produce the same result.

Warning

Deterministic operations are often slower than nondeterministic operations, so
single-run performance may decrease for your model. However, determinism may
save time in development by facilitating experimentation, debugging, and
regression testing.

## Controlling sources of randomness¶

### PyTorch random number generator¶

You can use `torch.manual_seed()` to seed the RNG for all devices (both CPU
and CUDA):

    
    
    import torch
    torch.manual_seed(0)
    

Some PyTorch operations may use random numbers internally.
`torch.svd_lowrank()` does this, for instance. Consequently, calling it
multiple times back-to-back with the same input arguments may give different
results. However, as long as `torch.manual_seed()` is set to a constant at the
beginning of an application and all other sources of nondeterminism have been
eliminated, the same series of random numbers will be generated each time the
application is run in the same environment.

It is also possible to obtain identical results from an operation that uses
random numbers by setting `torch.manual_seed()` to the same value between
subsequent calls.

### Python¶

For custom operators, you might need to set python seed as well:

    
    
    import random
    random.seed(0)
    

### Random number generators in other libraries¶

If you or any of the libraries you are using rely on NumPy, you can seed the
global NumPy RNG with:

    
    
    import numpy as np
    np.random.seed(0)
    

However, some applications and libraries may use NumPy Random Generator
objects, not the global RNG
(https://numpy.org/doc/stable/reference/random/generator.html), and those will
need to be seeded consistently as well.

If you are using any other libraries that use random number generators, refer
to the documentation for those libraries to see how to set consistent seeds
for them.

### CUDA convolution benchmarking¶

The cuDNN library, used by CUDA convolution operations, can be a source of
nondeterminism across multiple executions of an application. When a cuDNN
convolution is called with a new set of size parameters, an optional feature
can run multiple convolution algorithms, benchmarking them to find the fastest
one. Then, the fastest algorithm will be used consistently during the rest of
the process for the corresponding set of size parameters. Due to benchmarking
noise and different hardware, the benchmark may select different algorithms on
subsequent runs, even on the same machine.

Disabling the benchmarking feature with `torch.backends.cudnn.benchmark =
False` causes cuDNN to deterministically select an algorithm, possibly at the
cost of reduced performance.

However, if you do not need reproducibility across multiple executions of your
application, then performance might improve if the benchmarking feature is
enabled with `torch.backends.cudnn.benchmark = True`.

Note that this setting is different from the
`torch.backends.cudnn.deterministic` setting discussed below.

## Avoiding nondeterministic algorithms¶

`torch.use_deterministic_algorithms()` lets you configure PyTorch to use
deterministic algorithms instead of nondeterministic ones where available, and
to throw an error if an operation is known to be nondeterministic (and without
a deterministic alternative).

Please check the documentation for `torch.use_deterministic_algorithms()` for
a full list of affected operations. If an operation does not act correctly
according to the documentation, or if you need a deterministic implementation
of an operation that does not have one, please submit an issue:
https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22

For example, running the nondeterministic CUDA implementation of
`torch.Tensor.index_add_()` will throw an error:

    
    
    >>> import torch
    >>> torch.use_deterministic_algorithms(True)
    >>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))
    Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
    RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set
    'torch.use_deterministic_algorithms(True)'. ...
    

When `torch.bmm()` is called with sparse-dense CUDA tensors it typically uses
a nondeterministic algorithm, but when the deterministic flag is turned on,
its alternate deterministic implementation will be used:

    
    
    >>> import torch
    >>> torch.use_deterministic_algorithms(True)
    >>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())
    tensor([[[ 1.1900, -2.3409],
             [ 0.4796,  0.8003]],
            [[ 0.1509,  1.8027],
             [ 0.0333, -1.1444]]], device='cuda:0')
    

Furthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or
greater, you should set the environment variable CUBLAS_WORKSPACE_CONFIG
according to CUDA documentation:
https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

### CUDA convolution determinism¶

While disabling CUDA convolution benchmarking (discussed above) ensures that
CUDA selects the same algorithm each time an application is run, that
algorithm itself may be nondeterministic, unless either
`torch.use_deterministic_algorithms(True)` or
`torch.backends.cudnn.deterministic = True` is set. The latter setting
controls only this behavior, unlike `torch.use_deterministic_algorithms()`
which will make other PyTorch operations behave deterministically, too.

### CUDA RNN and LSTM¶

In some versions of CUDA, RNNs and LSTM networks may have non-deterministic
behavior. See `torch.nn.RNN()` and `torch.nn.LSTM()` for details and
workarounds.

## DataLoader¶

DataLoader will reseed workers following Randomness in multi-process data
loading algorithm. Use `worker_init_fn()` and generator to preserve
reproducibility:

    
    
    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % 2**32
        numpy.random.seed(worker_seed)
        random.seed(worker_seed)
    
    g = torch.Generator()
    g.manual_seed(0)
    
    DataLoader(
        train_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        worker_init_fn=seed_worker,
        generator=g,
    )
    

Next ![](../_static/images/chevron-right-orange.svg)
![](../_static/images/chevron-right-orange.svg) Previous

* * *

(C) Copyright 2023, PyTorch Contributors.

Built with Sphinx using a theme provided by Read the Docs.

  * Reproducibility
    * Controlling sources of randomness
      * PyTorch random number generator
      * Python
      * Random number generators in other libraries
      * CUDA convolution benchmarking
    * Avoiding nondeterministic algorithms
      * CUDA convolution determinism
      * CUDA RNN and LSTM
    * DataLoader

![](https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&guid=ON&script=0)

## Docs

Access comprehensive developer documentation for PyTorch

View Docs

## Tutorials

Get in-depth tutorials for beginners and advanced developers

View Tutorials

## Resources

Find development resources and get your questions answered

View Resources

  * PyTorch
  * Get Started
  * Features
  * Ecosystem
  * Blog
  * Contributing

  * Resources
  * Tutorials
  * Docs
  * Discuss
  * Github Issues
  * Brand Guidelines

  * Stay up to date
  * Facebook
  * Twitter
  * YouTube
  * LinkedIn

  * PyTorch Podcasts
  * Spotify
  * Apple
  * Google
  * Amazon

  * Terms
  * |
  * Privacy

© Copyright The Linux Foundation. The PyTorch Foundation is a project of The
Linux Foundation. For web site terms of use, trademark policy and other
policies applicable to The PyTorch Foundation please see
www.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch
open source project, which has been established as PyTorch Project a Series of
LF Projects, LLC. For policies applicable to the PyTorch Project a Series of
LF Projects, LLC, please see www.lfprojects.org/policies/.

To analyze traffic and optimize your experience, we serve cookies on this
site. By clicking or navigating, you agree to allow our usage of cookies. As
the current maintainers of this site, Facebook’s Cookies Policy applies. Learn
more, including about available controls: Cookies Policy.

![](../_static/images/pytorch-x.svg)

  * Get Started
  * Ecosystem
  * Mobile
  * Blog
  * Tutorials
  * Docs 
    * PyTorch
    * torchaudio
    * torchtext
    * torchvision
    * torcharrow
    * TorchData
    * TorchRec
    * TorchServe
    * TorchX
    * PyTorch on XLA Devices
  * Resources 
    * About
    * PyTorch Foundation
    * Community
    * Community Stories
    * Developer Resources
    * Events
    * Forums
    * Models (Beta)
  * Github

